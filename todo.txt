#################   MAIN APP TODOS
########################################################################################################################

####  basic UI friendliness
- utilize `clickpos1` more like a selector

####  basic error handling / edge cases / expanding configurables
- better defaults / init for `modelData`
- connect the `modelData.feature` (or just `modelData` in general), table in gui, and conf.selectedFeatures
- make `modelData.centers` configurable to more than '30';

####  holes in functionality (code defensiveness?)
- at gui start, pre-make (if nec) all `data/` dir structure

####  housekeeping / tech-debt
- detect class from modelClassifierFile to get conf.classifier




#################   NOTES TO REMEMBER NEXT STEPS
########################################################################################################################

####  branch model_gui:
- implement `pushbutton_save_Callback`, feature extraction & save the data as class
    - revise `getModelClass.m`




#################   MISC NOTES
########################################################################################################################


####  frigui travel:
may 13 - 23; july 8 - 22;


####  emotion todo:
angry & neutral testing, verbal vs. nonverbal?


####  paper - latex, university template file - look online
table of contents

chapter 1 - intro
highlight importance

chapter 2 - related work
- general stuff about audio features (similar to shuangs 'related work')
- section that describes shuangs
- differences / similarities   - comparison

chapter 3 - my work


####  defense committee
check last day to submit thesis, work backwards
defend 3 or 4 days before that
10-14 weeks have thesis ready
before that, figure out committee stuff



#################   2017-04-17
########################################################################################################################
- use LPCC
- normalized vs. unnorm
- verbal+nonverbal vs. just verbal

- speaker via hue, emotion via saturation

-features? https://link.springer.com/article/10.1186/1687-4722-2012-16




- all norm
- all unnorm
- no-neutral-nonverbal norm
- no-neutral-nonverbal unnorm
- verbal norm
- verbal unnorm


sox normalized/*.wav emo_all_norm_100pct.wav
sox unnormalized/*.wav emo_all_unnorm_100pct.wav

sox normalized/*.wav emo_NoNeutNonv_norm_100pct.wav
sox unnormalized/*.wav emo_NoNeutNonv_unnorm_100pct.wav

sox normalized/*_verbal*.wav emo_verbal_norm_100pct.wav
sox unnormalized/*_verbal*.wav emo_verbal_unnorm_100pct.wav









@(hObject,eventdata)medsim('pushbutton_emotion_classifier_Callback',hObject,eventdata,guidata(hObject))







Aaron Calhoun








Hello Dr. Calhoun, my name is Justin Taylor, and I'm a CS graduate student working on a Masters thesis on an extension to Shuang Jiang's dissertation on speech data analysis of audio/video of doctor/patient simulations.  His dissertation (which he defended a couple years ago May 2015) included an apparatus to analyze conversations between doctors and clients, including speaker identification, silence detection, and emotion classification.  My extension has focused on duplicating some of the speaker identification features, as well as deeper focus on the emotion detection.

It is my understanding that you had helped in part by providing video (perhaps DVD or raw video files) of these simulated engagements (I think using actors).  I'm approaching the light at the end of the tunnel on my work on this, and am wondering if you happen to have some raw video data (e.g. DVD, .avi, etc) for me to use - in particular where there are one or more actors that appear in multiple videos.  If that can't be found, any available, original video would still be useful for me.



  I'm wondering if you would happen to have     I'm approaching light at the end of the tunnel as far as my work in this, and am wondering if you have 